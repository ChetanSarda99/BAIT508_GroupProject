{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BAIT 508 Group Project: Industry Analysis\n",
    "\n",
    "PROJECT OVERVIEW: \n",
    "The goal of this project is to conduct an in-depth analysis of public US firms within selected industry sector(s) using various data analyses and natural language processing (NLP) techniques that we learned in BAIT 508. Each team will choose at least one industry sector to investigate and utilize multiple datasets to extract valuable industry insights from the data.\n",
    "\n",
    "The project will utilize three datasets (located in the data folder):\n",
    "•\tpublic_firms.csv\n",
    "•\tmajor_groups.csv\n",
    "•\t2020_10K_item1_full.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import seaborn as sns\n",
    "import matplotlib.style as style\n",
    "import string\n",
    "from DocumentSimilarity import DocumentSimilarity\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_data= r'C:\\Users\\sarda\\Desktop\\UBC\\BAIT 508\\Group_Project_BAIT508\\data'\n",
    "public_firms=pd.read_csv(r'C:\\Users\\sarda\\Desktop\\UBC\\BAIT 508\\Group_Project_BAIT508\\data\\public_firms.csv')\n",
    "twenty20_10K_item1_full=pd.read_csv(r'C:\\Users\\sarda\\Desktop\\UBC\\BAIT 508\\Group_Project_BAIT508\\data\\2020_10K_item1_full.csv')\n",
    "major_groups =pd.read_csv(r'C:\\Users\\sarda\\Desktop\\UBC\\BAIT 508\\Group_Project_BAIT508\\data\\major_groups.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1. Quantitative Analysis of the Industry Sector\n",
    "\n",
    "A.\t[Industry Sector Selection and Data Filtering; 20 points] \n",
    "\n",
    "1.\tThe file \"data/major_groups.csv\" contains a list of major industry sectors and their corresponding codes (column \"major_group\"). Your first task is to choose at least one industry sector that interests your group. It is okay if multiple groups choose the same industry sector, so you don’t need to coordinate with other groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agri = ( '1','2','7','8','9') #Agriculture and Related Industries\n",
    "wholesale = ( '50','51') #Wholesale\n",
    "food =('54') #Food\n",
    "busi_serv = ('73') #BusinessSerivices\n",
    "wholesale_only = ('50') #Wholesale_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tNext, filter the data in \"data/public_firms.csv\" to only include the firms belonging to the industry sector(s) you have selected. You can use the \"major_group\" value, which corresponds to the first two digits of each firm’s SIC code,  to identify relevant firms. For example, if you are interested in the “Business Service” sector and its \"major_group\" code is 73, you should retain all firms whose SIC codes start with 73. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industries_agri = public_firms[public_firms['sic'].astype(str).str.startswith(agri)]\n",
    "industries_wholesale = public_firms[public_firms['sic'].astype(str).str.startswith(wholesale)]\n",
    "industries_food = public_firms[public_firms['sic'].astype(str).str.startswith(food)]\n",
    "industries_busi_serv = public_firms[public_firms['sic'].astype(str).str.startswith(busi_serv)]\n",
    "industries_wholesale_only =public_firms[public_firms['sic'].astype(str).str.startswith(wholesale_only)]\n",
    "fname_industries = industries_wholesale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tNow, answer the following questions based on the filtered dataset: \n",
    "\n",
    "a.\tHow many unique firm-year (\"fyear\") observations are there in the filtered dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_fyear=len(fname_industries['fyear'].unique())\n",
    "unique_fyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.\tHow many unique firms are there in the filtered dataset?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_firms = len(fname_industries['conm'].unique())\n",
    "unique_firms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.\tHow many firms in the filtered dataset have records over all 27 years (1994-2020)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_firms = fname_industries.groupby('conm').filter(lambda x: len(x) == unique_fyear)\n",
    "num_firms = len(filtered_firms['conm'].unique())\n",
    "num_firms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B.\t[Preliminary Analysis; 20 points] Answer the following questions:\n",
    "\n",
    "1.\tWhat are the top 10 firms with the highest stock price (column \"prcc_c\") in the year 2020?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_stocks = fname_industries.sort_values('prcc_c', ascending=False).loc[fname_industries['fyear'] == 2020].head(10)\n",
    "top_ten_stocks[['conm']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat are the top 10 firms with the highest sales (column \"sale\") in the entire history of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_firm = fname_industries.groupby('conm').sum(numeric_only=True).sort_values('sale', ascending=False).head(10)\n",
    "top_10_firm[['sale']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tWhat is the geographical distribution (column \"location\") of all the firms? In other words, how many firms are there in each location? Please list the top 10 locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_locations = fname_industries.groupby('location')['conm'].nunique().sort_values(ascending=False)\n",
    "unique_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tCreate a line chart to show the average stock price (column \"prcc_c\") in the selected sector(s) across the years. If you have selected multiple sectors, draw multiple lines to show them separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_prcc_c = fname_industries.groupby(['sic', 'fyear'])['prcc_c'].mean()\n",
    "\n",
    "#for i, industry in enumerate(fname_industries['sic'].unique()):\n",
    "  #  industry_data = mean_prcc_c.loc[industry]\n",
    "  #  axes[i].plot(industry_data.index, industry_data.values)\n",
    "  #  axes[i].set_xlabel('')\n",
    "  #  axes[i].set_xticks([])\n",
    "  #  axes[i].set_ylabel('')\n",
    "  #  axes[i].set_title(f'Industry_{industry}')\n",
    "\n",
    "mean_prcc_c = fname_industries.groupby(['fyear'])['prcc_c'].mean()\n",
    "style.use('bmh')\n",
    "\n",
    "plt.plot(mean_prcc_c.index, mean_prcc_c.values)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mean prcc_c') \n",
    "plt.title('Mean prcc_c by Year')\n",
    "plt.show()\n",
    "#https://stackoverflow.com/questions/63319572/plotting-two-columns-based-on-multiple-variables-in-another-column\n",
    "#Helpful to plot prcc_c for different industry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tWhich firm was affected the most by the 2008 Financial Crisis, as measured by the percentage drop in stock price from 2007 to 2008?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prcc_c_2008 = fname_industries[fname_industries['fyear'] == 2008]\n",
    "prcc_c_2007 = fname_industries[fname_industries['fyear'] == 2007]\n",
    "prcc_merged = prcc_c_2008.merge(prcc_c_2007,\n",
    "                 how='outer', suffixes=('_x', '_y'), on = 'gvkey')\n",
    "prcc_merged.rename(columns={'prcc_c_x': 'prccc2008', 'prcc_c_y': 'prccc2007','location_x':'location','conm_x':'conm'}, inplace=True)\n",
    "prcc_merged.drop(prcc_merged.filter(regex=('_'),).columns, axis=1, inplace=True)\n",
    "prcc_merged.dropna(inplace=True)\n",
    "prcc_merged.rename(columns={'prccc2008': 'prcc_c_2008', 'prccc2007': 'prcc_c_2007'}, inplace=True)\n",
    "prcc_merged['prcc_c_drop'] = (prcc_merged['prcc_c_2008'] - prcc_merged['prcc_c_2007'])*100/prcc_merged['prcc_c_2007']\n",
    "prcc_merged.sort_values('prcc_c_drop', inplace=True)\n",
    "prcc_merged.reset_index(drop=True,inplace=True)\n",
    "most_affected_firm = prcc_merged.iloc[[0]]\n",
    "most_affected_firm\n",
    "#https://stackoverflow.com/questions/19125091/pandas-merge-how-to-avoid-duplicating-columns\n",
    "#Initially used to drop duplicates but code didn't work as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_industries.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tPlot the average Return on Assets (ROA) for the firms located in the “USA” across the years. ROA is calculated as ni/asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_industries_roa = fname_industries.copy()\n",
    "fname_industries_roa['roa'] = fname_industries['ni'] / fname_industries['asset']\n",
    "usa_roa_yearly_mean_plot = fname_industries_roa[fname_industries_roa['location'] == 'USA'].groupby('fyear')['roa'].mean().plot(legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2. Text Analysis on the Industry Sector\n",
    "\n",
    "C.\t[Text Cleaning; 10 points] The file \"data/2020_10K_item1_full.csv\" contains a sample of 5,988 firms and their “item 1” content in their 10-K reports in the year 2020.  Load the dataset as a DataFrame and create a new column containing the cleaned text for each “item1” content. Follow the steps below to clean the text:\n",
    "\n",
    "1.\tConvert all words to lowercase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty20_10K_item1_clean = twenty20_10K_item1_full.copy()\n",
    "#twenty20_10K_item1_clean.drop_duplicates(inplace=True)\n",
    "twenty20_10K_item1_clean['item_1_text'] = twenty20_10K_item1_full['item_1_text'].str.lower()\n",
    "twenty20_10K_item1_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tRemove punctuations.\n",
    "\n",
    "3.\tRemove stop words based on the list of English stop words in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "sw = stopwords.words('english')\n",
    "def clean_text(text):\n",
    "    clean_text = text.lower()\n",
    "    clean_text = clean_text.translate(translator)\n",
    "    clean_words = [w for w in clean_text.split() if w not in sw]\n",
    "    return ' '.join(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    twenty20_10K_item1_clean = pd.read_csv(r'C:\\Users\\sarda\\Desktop\\UBC\\BAIT 508\\Group_Project_BAIT508\\data\\twenty20_10K_item1_clean.csv')\n",
    "except:\n",
    "    twenty20_10K_item1_clean['item_1_clean'] = twenty20_10K_item1_clean['item_1_text'].apply(clean_text).reset_index(drop=True)\n",
    "    twenty20_10K_item1_clean.drop(['name'],axis = 1,inplace=True)\n",
    "    twenty20_10K_item1_clean.to_csv(r'C:\\Users\\sarda\\Desktop\\UBC\\BAIT 508\\Group_Project_BAIT508\\data\\twenty20_10K_item1_clean.csv',index=False)  \n",
    "\n",
    "#twenty20_10K_item1_clean['item_1_clean'] = twenty20_10K_item1_clean['item_1_text'].apply(clean_text).reset_index(drop=True)\n",
    "#twenty20_10K_item1_clean.drop(['name'],axis = 1,inplace=True)\n",
    "#twenty20_10K_item1_clean.to_csv(r'C:\\Users\\sarda\\Desktop\\UBC\\BAIT 508\\Group_Project_BAIT508\\data\\twenty20_10K_item1_clean.csv',index=False) \n",
    "twenty20_10K_item1_clean = pd.read_csv(r'C:\\Users\\sarda\\Desktop\\UBC\\BAIT 508\\Group_Project_BAIT508\\data\\twenty20_10K_item1_clean.csv')    \n",
    "twenty20_10K_item1_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.\t[Keyword Analysis; 20 points] Conduct keywords analysis on your selected industry sector(s). Follow the steps below to complete the analysis:\n",
    "\n",
    "1.\tCreate a new DataFrame that includes only firms in your selected industry sectors. Ensure that you merge the 10-K data with the previous \"public_firm.csv\" data using an inner join.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_firms_2020 = fname_industries[fname_industries['fyear'] == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pf_i1_2020 = public_firms_2020.merge(twenty20_10K_item1_clean, how='inner', on='gvkey')\n",
    "merged_pf_i1_2020.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tGenerate the top 10 keywords for each firm based on two different methods: word counts and TF-IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_wc_top10(text):\n",
    "    c = Counter(text.lower().split())\n",
    "    words = []\n",
    "    for pair in c.most_common(10):\n",
    "        words.append(pair[0])\n",
    "    return ' '.join([x[0] for x in c.most_common(10)])\n",
    "def get_keywords_tfidf_top10(document_list):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(document_list)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_keywords = []\n",
    "    for i in range(len(document_list)):\n",
    "        feature_index = tfidf_matrix[i, :].nonzero()[1]\n",
    "        tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
    "        sorted_tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "        top_keywords.append(' '.join([feature_names[i] for i, _ in sorted_tfidf_scores[:10]]))\n",
    "    return top_keywords\n",
    "\n",
    "\n",
    "merged_pf_i1_2020['keyword_wc_top10'] = merged_pf_i1_2020.item_1_clean.apply(get_keywords_wc_top10)\n",
    "merged_pf_i1_2020['keyword_tfidf_top10'] = get_keywords_tfidf_top10(merged_pf_i1_2020.item_1_clean.tolist())\n",
    "\n",
    "merged_pf_i1_2020.to_csv(r'C:\\Users\\sarda\\Desktop\\UBC\\BAIT 508\\Group_Project_BAIT508\\data\\merged_pf_i1_2020.csv',index=False)\n",
    "merged_pf_i1_2020.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tCreate two wordclouds to visualize the keywords across all firms in the selected sector: one based on the word count of keywords and another based on the TF-IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_wc = WordCloud(width=800, height=400,background_color='#b4d5FF',colormap='Dark2', max_font_size=130).generate(' '.join(merged_pf_i1_2020.keyword_wc_top10.tolist()))\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud_wc)\n",
    "plt.savefig(r'C:\\Users\\sarda\\Desktop\\UBC\\BAIT 508\\Group_Project_BAIT508\\repo_data\\imgs\\keyword_wc.png')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_tfidf = WordCloud(width=800, height=400, background_color='#b4d5FF',colormap='Dark2', max_font_size=130).generate(' '.join(merged_pf_i1_2020.keyword_tfidf_top10.tolist()))\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud_tfidf)\n",
    "plt.savefig(r'C:\\Users\\sarda\\Desktop\\UBC\\BAIT 508\\Group_Project_BAIT508\\repo_data\\imgs\\keyword_tfidf.png') # save as PNG file\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.\t[Word embedding; 20 points] Train a word2vec model and analyze word similarities.\n",
    "1.\tTrain a word2vec model with the full 10-K sample (e.g., \"data/2020_10K_item1_full.csv\"). Please use the cleaned text (e.g., results from Step C) for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = Word2Vec.load(r'C:\\Users\\sarda\\Desktop\\UBC\\BAIT 508\\Group_Project_BAIT508\\repo_data\\models\\word2vec.model')\n",
    "except:\n",
    "    sent = [row.split() for row in twenty20_10K_item1_clean['item_1_clean']]\n",
    "    model = Word2Vec(sent, min_count=1, vector_size=50, workers=3, window=3, sg = 1)\n",
    "    model.save(r'C:\\Users\\sarda\\Desktop\\UBC\\BAIT 508\\Group_Project_BAIT508\\repo_data\\models\\word2vec.model')\n",
    "    model = Word2Vec.load(r'C:\\Users\\sarda\\Desktop\\UBC\\BAIT 508\\Group_Project_BAIT508\\repo_data\\models\\word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tManually inspect the wordclouds you generated in D.3 and choose three representative keywords that are relevant to the industry sector of your interest. Utilize the trained word2vec model to find the most relevant five words for each of these three keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words_top10_wc = ['distribution','sales','service'] #Related to Wholesale\n",
    "words_top10_tfidf = ['reseller','sales','service'] #Related to Wholesale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in words_top10_tfidf:\n",
    "    print(f'Similar words for {_}: {model.wv.most_similar(positive=_)[:5]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3. Comprehensive Analysis of One Sample Firm\n",
    "F.\t[Firm Analysis and Strategy Suggestion; 10 points] This is an open question. Pick one firm that you are interested in and try to analyze its market status. The ultimate goal is to provide one valuable suggestion to the firm based on your analysis. Some directions you might consider are, but not limited to:\n",
    "1.\tConvert the keywords extracted in D.2 into word embeddings with the word2vec model trained in E.1. Add up the embeddings for each firm to create the firm-level embeddings. Use the firm-level embeddings to find the focal firm’s competing firms (or, most similar firms). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsimilar = DocumentSimilarity(model = model, gvkeys=merged_pf_i1_2020['gvkey'], conm = merged_pf_i1_2020['conm'], \n",
    "                       keywordslist = merged_pf_i1_2020['keyword_tfidf_top10'])\n",
    "firm = merged_pf_i1_2020.loc[merged_pf_i1_2020['sale'].idxmax(), 'gvkey']\n",
    "dsimilar.get_firm_embedding(firm)\n",
    "most_similar_firms_ = dsimilar.most_similar(firm, topn = 5)\n",
    "most_similar_firms_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tCompare the revenue, market share, and ROA of the focal firm to its competitors and provide suggestions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firms = []\n",
    "for _ in most_similar_firms_:\n",
    "    firms.append(firm)\n",
    "    firms.append(_[0])\n",
    "most_similar_firms = merged_pf_i1_2020[merged_pf_i1_2020['gvkey'].isin(firms)].copy()\n",
    "most_similar_firms['revenue'] = most_similar_firms['sale']\n",
    "most_similar_firms['market_share'] = most_similar_firms['sale'] / most_similar_firms['sale'].sum()\n",
    "most_similar_firms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_plot = ['revenue','roa', 'market_share']\n",
    "for col in columns_to_plot:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    sns.barplot(x=col, y='conm', data=most_similar_firms)\n",
    "    plt.savefig(f'imgs\\\\{col}.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tPerform an analysis of the historical stock prices, ROA, revenue, and assets of the chosen company. Investigate potential correlations and address noteworthy decreases and increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data_for_firm = public_firms[public_firms['gvkey'] == firm].copy()\n",
    "historical_data_for_firm\n",
    "\n",
    "sns.lineplot(x='fyear', y='sale', data=historical_data_for_firm)\n",
    "plt.xlabel('Fiscal Year')\n",
    "plt.ylabel('Revenue')\n",
    "plt.title('Historical Revenue for the Firm')\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(x='fyear', y='roa', data=historical_data_for_firm)\n",
    "plt.xlabel('Fiscal Year')\n",
    "plt.ylabel('Return on Assets')\n",
    "plt.title('Historical ROA for the Firm')\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(x='fyear', y='prcc_c', data=historical_data_for_firm)\n",
    "plt.xlabel('Fiscal Year')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.title('Historical Stock Price for the Firm')\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(x='fyear', y='asset', data=historical_data_for_firm)\n",
    "plt.xlabel('Fiscal Year')\n",
    "plt.ylabel('Assets')\n",
    "plt.title('Historical Assets value for the Firm')\n",
    "plt.show()\n",
    "\n",
    "sns.heatmap(historical_data_for_firm[['prcc_c', 'ch', 'ni', 'asset', 'sale', 'roa']].corr(numeric_only=True), annot=True)\n",
    "# reference - https://blog.quantinsti.com/creating-heatmap-using-python-seaborn/#:~:text=A%20heatmap%20is%20a%20two,as%20per%20the%20creator's%20requirement.\n",
    "# to create heatmap with correlation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
